{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rziXcUSBskS3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Developer Account\n",
    "In order to use Twitter’s API, we have to create a developer account on the Twitter apps site.\n",
    " * Log in or make a Twitter account at https://apps.twitter.com/.\n",
    " * Create a new app (button on the top right)\n",
    " \n",
    "<img src=https://miro.medium.com/max/1400/0*Dq78m3JKoSqZY5SS.png style=\"width: 200px;\">\n",
    "\n",
    "Fill in the app creation page with a unique name, a website name (use a placeholder website if you don’t have one), and a project description. Accept the terms and conditions and proceed to the next page.\n",
    "\n",
    "Once your project has been created, click on the “Keys and Access Tokens” tab. You should now be able to see your consumer secret and consumer key.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/0*YU1pFqTw6Dn-ZmOd.png style=\"width: 200px;\">\n",
    "\n",
    "You’ll also need a pair of access tokens. Scroll down and request those tokens. The page should refresh, and you should now have an access token and access token secret.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/0*_gnOgA0aaAqPgDJG.png style=\"width: 200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UI2fIQFxrNLB",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pf5Xapqrq0M"
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "\n",
    "#install tweepy if you don't have it\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#sentiment analysis package\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#tweet pre-processor \n",
    "# !pip install tweet-preprocessor\n",
    "import preprocessor as p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting code\n",
    "Below we define some starting codes (python classes and function) to illustrate and assist on how to fetch data from twitter and analyse them. \n",
    "\n",
    "### **Your task is**\n",
    "1. Go through the code and understand it. Know what each function does\n",
    "2. If you find error, fix it. Ask for help in the slack channel if you find serious mistake\n",
    "3. Extend the code such that it will be useful for topics you choose to analyse\n",
    "4. Make nice plots and share your finding (e.g.  insight on the main covid19 twitter converstions about your country)\n",
    "5. Submit what ever you managed to do by Wednesday morning. But you should keep using what you build to write blogs, share on facebook, etc. \n",
    "\n",
    "\n",
    "[Reference used to build some of the functions here](https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "            consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "            access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "            access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter \n",
    "            #Streaming API\n",
    "#             auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "#             auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "            auth = OAuthHandler('Dl1sJGGSyHg45rl5cvGRFqB4q', '2AWIDJ89G4xFR5E2ggg6MEWvMKghupDzchE7XqIlEF5QeeQKbN')\n",
    "            auth.set_access_token('1064907352963522561-Rn0VyQSiBZCVy22JiqjGNVE0c8XfAr', '97gl0daSxBWcIIgoIdOzuXSSsWAlrZZHw2toCaIBD69wV')\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True)            \n",
    "            \n",
    "\n",
    "    def clean_tweets(self,twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        self.filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=200, include_rts=False).pages():\n",
    "\n",
    "\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #filter by language\n",
    "                if status['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                clean_text = p.clean(status['text'])\n",
    "                filtered_tweet = self.clean_tweets(clean_text)\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'],filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for \\\n",
    "                                      hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for \\\n",
    "                                      mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search twitter and fetch data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_keywords = '#COVID19 OR #COVID19Africa'  #hashtag based search\n",
    "tweets_file = 'covid19_23june2020.json'\n",
    "\n",
    "#get data on keywords\n",
    "ts = tweetsearch()\n",
    "# df = ts.get_tweets(covid_keywords, csvfile=tweets_file)    #you saved the \n",
    "df = ts.get_tweets(covid_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream data and save it to file\n",
    "In the above we saw how to search and fetch data, below we will see how we will stream data from twitter. Make sure you understand the difference between search and stream features of twitter api.\n",
    "\n",
    "### **SAME TASK AS ABOVE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6lcy009rX_e"
   },
   "outputs": [],
   "source": [
    "#This is a basic listener that writes received tweets to file.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self,fhandle, stop_at = 1000):\n",
    "        self.tweet_counter = 0\n",
    "        self.stop_at = stop_at\n",
    "        self.fhandle = fhandle\n",
    "         \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.fhandle.write(data)\n",
    "        \n",
    "        #stop if enough tweets are obtained\n",
    "        self.tweet_counter += 1   \n",
    "        if self.tweet_counter < self.stop_at:        \n",
    "            return True\n",
    "        else:\n",
    "            print('Max number of tweets reached: #tweets = ' + str(self.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "def stream_tweet_data(filename='data/tweets.json',\n",
    "                      keywords=['COVID19Africa','COVID19Ethiopia'],\n",
    "                      is_async=False):\n",
    "    # tweet topics to use as a filter. The tweets downloaded\n",
    "    # will have one of the topics in their text or hashtag \n",
    "\n",
    "    print('saving data to file: ',filename)\n",
    "\n",
    "    #print the tweet topics \n",
    "    print('TweetKeywords are: ',keywords)\n",
    "    print('For testing case, please interupt the downloading process using ctrl+x after about 5 mins ')\n",
    "    print('To keep streaming in the background, pass is_async=True')\n",
    "\n",
    "    #Variables that contains the user credentials to access Twitter API \n",
    "    consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "    consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "    #open file \n",
    "    fhandle=open(filename,'w')\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener(fhandle)\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: first argument to this code\n",
    "    stream.filter(track=keywords,is_async=is_async)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case of the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8tcPcSMrNLL",
    "outputId": "d7abd9c2-065c-40e8-f71b-e808d985c364"
   },
   "outputs": [],
   "source": [
    "tweets_file = 'data/covid19_23june2020.json'\n",
    "stream_tweet_data(filename=tweets_file,keywords=['covid19','#COVID19Africa'])  #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter twitter data and do basic analysis\n",
    "**Extend it to gain more insight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8tcPcSMrNLL",
    "outputId": "d7abd9c2-065c-40e8-f71b-e808d985c364"
   },
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for line in open(tweets_file, \"r\"):\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        x=tweet['text']\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "print('saved numbers of tweets: ', len(tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlFKyGnYrNLX"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(columns=['text','lang','country'])\n",
    "\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, \n",
    "                             tweets_data))\n",
    "\n",
    "\n",
    "tweets_by_lang = tweets['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEPPoCBtrNLd",
    "outputId": "bfe0cee1-814d-4f30-f47d-205218b3adc9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar', color='red')\n",
    "\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQYFg6t5rNLj"
   },
   "source": [
    "# Hashtag histogram. \n",
    "\n",
    "## Please write code that will help you answer the following questions\n",
    " 1) What is the most used hashtag?\n",
    " \n",
    " 2) What is the most used referenced username?\n",
    " \n",
    " 3) What is the most retweeted tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0VUwYR8rNLk"
   },
   "outputs": [],
   "source": [
    "# getting the most used hashtag\n",
    "tags = df['hashtags']\n",
    "hashtag_counts = tags.value_counts()\n",
    "print(\"The most used hashtag is: \", tags[hashtag_counts.iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the most used referenced username\n",
    "names = df['user_mentions']\n",
    "username_counts = names.value_counts()\n",
    "print(\"The most used referenced username is: \", names[username_counts.iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['original_text']\n",
    "retweets = df['retweet_count']\n",
    "print(\"The most retweeted tweet is: \", tweets[retweet.max()])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vistweet.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
